{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1ZPSeolXVVIR4YJDdMaIb506MmgaFrldd",
      "authorship_tag": "ABX9TyN2NRuG2Mj0rmxLDlNZ87B8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IrfanSadik13/sherlock_gpt/blob/main/sherlock_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "\n",
        "# Dictionary of all Sherlock Holmes stories with their Gutenberg URLs\n",
        "sherlock_stories = {\n",
        "    \"A Study in Scarlet\": \"https://www.gutenberg.org/files/1661/1661-0.txt\",\n",
        "    \"The Sign of the Four\": \"https://www.gutenberg.org/files/2097/2097-0.txt\",\n",
        "    \"The Adventures of Sherlock Holmes\": \"https://www.gutenberg.org/files/1661/1661-0.txt\",\n",
        "    \"The Memoirs of Sherlock Holmes\": \"https://www.gutenberg.org/files/834/834-0.txt\",\n",
        "    \"The Hound of the Baskervilles\": \"https://www.gutenberg.org/files/2852/2852-0.txt\",\n",
        "    \"The Return of Sherlock Holmes\": \"https://www.gutenberg.org/files/108/108-0.txt\",\n",
        "    \"The Valley of Fear\": \"https://www.gutenberg.org/files/22357/22357-0.txt\",\n",
        "    \"His Last Bow\": \"https://www.gutenberg.org/files/2852/2852-0.txt\",\n",
        "    \"The Case-Book of Sherlock Holmes\": \"https://www.gutenberg.org/files/221/221-0.txt\"\n",
        "}\n",
        "\n",
        "# Output file to store all collected text\n",
        "output_file = \"sherlock_holmes_complete.txt\"\n",
        "\n",
        "# Fetch and save stories\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for title, url in sherlock_stories.items():\n",
        "        print(f\"Fetching {title}...\")\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            f.write(f\"\\n\\n=== {title} ===\\n\\n\")\n",
        "            f.write(response.text)\n",
        "        else:\n",
        "            print(f\"Failed to fetch {title}\")\n",
        "\n",
        "print(f\"All Sherlock Holmes stories saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID-bQh2KXmCL",
        "outputId": "9ea55029-b0e8-4372-f67b-ac313958d6c1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching A Study in Scarlet...\n",
            "Fetching The Sign of the Four...\n",
            "Fetching The Adventures of Sherlock Holmes...\n",
            "Fetching The Memoirs of Sherlock Holmes...\n",
            "Fetching The Hound of the Baskervilles...\n",
            "Fetching The Return of Sherlock Holmes...\n",
            "Fetching The Valley of Fear...\n",
            "Failed to fetch The Valley of Fear\n",
            "Fetching His Last Bow...\n",
            "Fetching The Case-Book of Sherlock Holmes...\n",
            "All Sherlock Holmes stories saved to sherlock_holmes_complete.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "\n",
        "# Load the collected text file\n",
        "input_file = \"sherlock_holmes_complete.txt\"\n",
        "output_file = \"sherlock_cleaned.txt\"\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove Gutenberg headers and footers\n",
        "    text = re.sub(r\"\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK .*? \\*\\*\\*\", \"\", text, flags=re.DOTALL)\n",
        "    text = re.sub(r\"\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK .*? \\*\\*\\*\", \"\", text, flags=re.DOTALL)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "\n",
        "    # Normalize text (optional)\n",
        "    text = text.lower()\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# Read the file and clean it\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "cleaned_text = clean_text(raw_text)\n",
        "\n",
        "# Save the cleaned text\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(cleaned_text)\n",
        "\n",
        "print(f\"Preprocessed text saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KLNyJqbYoD7",
        "outputId": "6d7d393d-29ba-406b-fc82-9b017c1b090e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed text saved to sherlock_cleaned.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install tokenizers\n",
        "\n",
        "from tokenizers import Tokenizer, trainers, models, pre_tokenizers\n",
        "\n",
        "# Load text\n",
        "with open(\"sherlock_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Train a new Byte-Pair Encoding (BPE) tokenizer\n",
        "tokenizer = Tokenizer(models.BPE())\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "trainer = trainers.BpeTrainer(vocab_size=8000, min_frequency=2)\n",
        "\n",
        "# Train tokenizer\n",
        "tokenizer.train_from_iterator([text], trainer)\n",
        "\n",
        "# Test tokenization\n",
        "encoded = tokenizer.encode(\"Mr. Sherlock Holmes was sitting in his chair.\")\n",
        "print(\"Tokens:\", encoded.ids)\n",
        "print(\"Decoded:\", tokenizer.decode(encoded.ids))\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save(\"sherlock_tokenizer.json\")\n",
        "print(\"Tokenizer saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ccy4mpoasQU",
        "outputId": "fed6c2a5-8f54-4c25-af36-8b1b4983986f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTokens: [49, 12, 216, 388, 466, 183, 120, 1364, 86, 129, 702, 12]\n",
            "Decoded: r . her lock ol mes was sitting in his chair .\n",
            "Tokenizer saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Load tokenized data\n",
        "with open(\"sherlock_tokenizer.json\", \"r\") as f:\n",
        "    from tokenizers import Tokenizer\n",
        "    tokenizer = Tokenizer.from_file(\"sherlock_tokenizer.json\")\n",
        "\n",
        "# Load the cleaned text\n",
        "with open(\"sherlock_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Tokenize the text\n",
        "encoded_text = tokenizer.encode(text).ids\n",
        "\n",
        "# Define sequence length\n",
        "SEQ_LEN = 256  # You can adjust this based on GPU memory\n",
        "\n",
        "# Create input-output pairs\n",
        "input_sequences = []\n",
        "output_sequences = []\n",
        "\n",
        "for i in range(len(encoded_text) - SEQ_LEN):\n",
        "    input_sequences.append(encoded_text[i:i+SEQ_LEN])\n",
        "    output_sequences.append(encoded_text[i+1:i+SEQ_LEN+1])\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X = torch.tensor(input_sequences, dtype=torch.long)\n",
        "Y = torch.tensor(output_sequences, dtype=torch.long)\n",
        "\n",
        "# Save as PyTorch dataset\n",
        "torch.save((X, Y), \"sherlock_train_data.pt\")\n",
        "\n",
        "print(\"Training data prepared and saved!\")"
      ],
      "metadata": {
        "id": "R_3RDZ6niXiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MiniGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, n_heads=4, num_layers=4, seq_len=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embedding = nn.Embedding(seq_len, d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.shape[1]\n",
        "        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0)\n",
        "\n",
        "        x = self.embedding(x) + self.pos_embedding(positions)\n",
        "        x = self.transformer(x)\n",
        "        x = self.fc_out(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Load tokenizer\n",
        "from tokenizers import Tokenizer\n",
        "tokenizer = Tokenizer.from_file(\"sherlock_tokenizer.json\")\n",
        "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
        "\n",
        "# Define model\n",
        "model = MiniGPT(VOCAB_SIZE)\n",
        "\n",
        "print(\"Model initialized!\")"
      ],
      "metadata": {
        "id": "CkjrzQLokXb7",
        "outputId": "bbfb1315-006b-48bb-a97d-13279a7b7555",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load dataset\n",
        "X, Y = torch.load(\"sherlock_train_data.pt\")\n",
        "\n",
        "# Set device (use GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move data to GPU\n",
        "X, Y = X.to(device), Y.to(device)\n",
        "\n",
        "# Create DataLoader\n",
        "BATCH_SIZE = 32\n",
        "dataset = TensorDataset(X, Y)\n",
        "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Initialize model & move to GPU\n",
        "VOCAB_SIZE = 8000  # Make sure to match vocab size with tokenizer\n",
        "model = MiniGPT(VOCAB_SIZE).to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Mixed precision scaler\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Training loop\n",
        "EPOCHS = 5  # Adjust as needed\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        inputs, targets = batch\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast():  # Mixed precision\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.view(-1, VOCAB_SIZE), targets.view(-1))\n",
        "\n",
        "        # Scale loss & backprop\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Save trained model\n",
        "torch.save(model.state_dict(), \"sherlock_gpt_model.pth\")\n",
        "print(\"✅ Model training complete & saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RceIABLvnrno",
        "outputId": "eb9c252d-2284-4b61-bcf2-4eeb58464d91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-1c4937ed50e9>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  X, Y = torch.load(\"sherlock_train_data.pt\")\n",
            "<ipython-input-2-1c4937ed50e9>:29: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "<ipython-input-2-1c4937ed50e9>:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():  # Mixed precision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "-G1_7dcnoEQC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}